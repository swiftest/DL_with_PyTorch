{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch与科学计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 如何判断一个对象是否为Tensor张量\n",
    "obj = np.arange(1,10)\n",
    "print(torch.is_tensor(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "obj1 = torch.Tensor(10)\n",
    "print(torch.is_tensor(obj1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 如何全局设置Tensor数据类型？\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.Tensor(2).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 如何判断一个对象是否为Pytorch Storage对象\n",
    "# torch.Storage is a contiguous, one-dimensional array of a single data type\n",
    "print(torch.is_storage(obj))\n",
    "storage = torch.DoubleStorage(10)\n",
    "print(torch.is_storage(storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# 如何获取Tensor中元素的个数\n",
    "a = torch.Tensor(3, 4)\n",
    "print(torch.numel(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[         nan, 1.98120e-321, 3.58501e-316, 6.94312e-310],\n",
      "        [5.43472e-323, 6.94299e-310, 6.94299e-310, 3.70549e-322],\n",
      "        [ 0.00000e+00,          nan, 6.94299e-310, 6.94296e-310],\n",
      "        [6.94299e-310, 5.88531e-320, 2.96439e-323, 6.94299e-310],\n",
      "        [6.94299e-310, 4.94066e-324,  0.00000e+00,          nan],\n",
      "        [6.94299e-310,  0.00000e+00, 5.92484e-320, 5.43472e-323],\n",
      "        [6.94299e-310, 6.94299e-310, 3.75490e-322,  0.00000e+00],\n",
      "        [         nan, 6.94299e-310, 6.94296e-310, 6.94299e-310],\n",
      "        [5.96041e-320, 2.96439e-323, 6.94299e-310, 6.94299e-310],\n",
      "        [4.94066e-324,  0.00000e+00,          nan, 6.94299e-310]])\n"
     ]
    }
   ],
   "source": [
    "# 如何设置打印选项？\n",
    "# precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None\n",
    "torch.set_printoptions(precision=5, threshold=100, linewidth=100, edgeitems=4)\n",
    "print(torch.DoubleTensor(10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如何创建单位矩阵\n",
    "torch.eye(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])\n",
      "tensor([[3.60753e-316,  0.00000e+00, 2.12200e-314],\n",
      "        [ 4.09311e+93, 5.48798e+247, 2.97504e+228]])\n"
     ]
    }
   ],
   "source": [
    "# 如何从numpy多维数组创建Tensor张量\n",
    "ta = np.arange(1, 20, 2)\n",
    "print(torch.from_numpy(ta))\n",
    "b = np.ndarray((2, 3))\n",
    "print(torch.from_numpy(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 5., 7., 9.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 如何创建等差数列\n",
    "# start / end / step\n",
    "print(torch.linspace(1, 9, 5, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.00000e+01, 1.00000e+02, 1.00000e+03, 1.00000e+04, 1.00000e+05], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 和linspace类似的logspace\n",
    "print(torch.logspace(1, 5, 5, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建元素全部为1的矩阵\n",
    "print(torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.61933, 0.33088, 0.14240],\n",
      "        [0.22942, 0.95433, 0.07148]])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建0,1均匀的随机矩阵，形状可以指定\n",
    "print(torch.rand(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.10735, -0.77052, -0.68622],\n",
      "        [-0.84563,  0.37656,  0.10483]])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建标准正太分布随机矩阵？形状可以指定\n",
    "print(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 7 0 8 1 5 2 6 3]\n",
      "tensor([8, 9, 7, 1, 3, 6, 5, 2, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建随机整数序列，如同numpy.random.permutation?\n",
    "print(np.random.permutation(10))\n",
    "print(torch.randperm(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 7]\n",
      "tensor([1, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建一个列表如同numpy中的arange?\n",
    "print(np.arange(1, 10, 3))\n",
    "print(torch.arange(1, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 如何创建一个全0的矩阵？\n",
    "print(torch.zeros(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引、切片、拼接及换位方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 如何将多个Tensor按照某一维度拼接起来？\n",
    "# torch.cat通过关键字参数dim指定按哪个维度拼接\n",
    "tensor = torch.ones(2, 3)\n",
    "print(torch.cat([tensor, tensor]))\n",
    "print(torch.cat([tensor, tensor, tensor], dim=1))\n",
    "# stack方法也可以进行tensor的拼接\n",
    "print(torch.stack([tensor, tensor], dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "# 如何将一个Tensor按照指定维度切片成n个分片？\n",
    "# 按照第二维度将tensor切分为5个tensor\n",
    "x = torch.ones(2, 10)\n",
    "print(torch.chunk(x, 5, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[33., 66.],\n",
       "        [88., 99.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如何按照索引进行元素的聚合？\n",
    "# 如何将元素33,66,88,99聚合在一起?\n",
    "# 第一个参数是源tensor,第二个参数为维度，第三个参数为索引\n",
    "x = torch.Tensor([[33, 66, 9], [1, 99, 88]])\n",
    "torch.gather(x, 1, torch.LongTensor([[0, 1], [2, 1]]))\n",
    "# torch.gather(x, 0, torch.LongTensor([[0, 1, 1], [1, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.84111, -0.39753, -0.10181, -0.87226, -0.09474,  0.36593,  0.79511],\n",
      "        [ 0.08496,  0.67426,  0.26090,  1.46092,  0.17352,  1.09947,  0.42727]])\n",
      "tensor([[-0.39753, -0.87226,  0.36593],\n",
      "        [ 0.67426,  1.46092,  1.09947]])\n",
      "tensor([[0.08496, 0.67426, 0.26090, 1.46092, 0.17352, 1.09947, 0.42727]])\n"
     ]
    }
   ],
   "source": [
    "# 如何按照索引选择目标数据？\n",
    "# 如何取出第2、4、6列数据，返回新的tensor\n",
    "x = torch.randn(2, 7)\n",
    "print(x)\n",
    "# 第一个参数为源tensor,第二个参数为维度，第三个参数为该维度上的索引\n",
    "y = torch.index_select(x, 1, torch.LongTensor([1, 3, 5]))\n",
    "print(y)\n",
    "z = torch.index_select(x, 0, torch.LongTensor([1]))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.34463, 0.83414, 0.39324, 0.93638],\n",
      "        [0.47847, 0.44576, 0.92530, 0.87828]])\n",
      "tensor([[False,  True, False,  True],\n",
      "        [False, False,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.83414, 0.93638, 0.92530, 0.87828])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如何选出满足条件的所有元素\n",
    "# masked_select方法，返回mask标志为1的所有元素组成的1维Tensor\n",
    "x = torch.rand(2, 4)\n",
    "print(x)\n",
    "# 判断元素是否大于0.5， 大于0.5为1， 小于或等于0.5的为0\n",
    "mask = x.ge(0.5)\n",
    "# mask = x.le(0.5)\n",
    "print(mask)\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.00000, 11.00000],\n",
      "         [66.00000, 88.00000]],\n",
      "\n",
      "        [[22.00000, 33.00000],\n",
      "         [ 0.00000,  0.10000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如何找出矩阵中非零元素的索引？\n",
    "# nonzero方法返回非零的索引，结果tensor为二维tensor，行数等于源tensor中非零元素个数，列数等于源tensor的维度(索引)\n",
    "x = torch.Tensor([[[0.0, 11], [66, 88]], [[22, 33], [0.0, 0.1]]])\n",
    "print(x)\n",
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "(tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]), tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]))\n",
      "(tensor([[1.],\n",
      "        [1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "# 如何将输入张量分割成相同形状的chunks?\n",
    "x = torch.ones(2, 10)\n",
    "print(x)\n",
    "# 每个块有5个元素，dim表示按第二维度\n",
    "print(torch.split(x, 5, dim=1))\n",
    "# 也可指定一个划分列表，依次表示有1,2,3,4个长度\n",
    "print(torch.split(x, [1, 2, 3, 4], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 如何给矩阵增加维度\n",
    "x = torch.Tensor([1, 2, 3, 4, 5, 6])\n",
    "# dim关键字参数指定在第几维度增加`[]`\n",
    "y = x.unsqueeze(dim=0)\n",
    "print(y)\n",
    "z = x.unsqueeze(dim=1)\n",
    "print(z)\n",
    "print(torch.unsqueeze(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  2.,  3.,  4.],\n",
      "         [22., 33., 44., 55.]]]) torch.Size([1, 2, 4])\n",
      "tensor([[ 0.,  2.,  3.,  4.],\n",
      "        [22., 33., 44., 55.]]) torch.Size([2, 4])\n",
      "tensor([[ 0.,  2.,  3.,  4.],\n",
      "        [22., 33., 44., 55.]]) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 如何去掉`[]`降低维度？去维度为1的\n",
    "x = torch.Tensor([[[0, 2, 3, 4], [22, 33, 44, 55]]])\n",
    "print(x, x.shape)\n",
    "y = torch.squeeze(x, dim=0)\n",
    "print(y, y.shape)\n",
    "z = x.squeeze(dim=0)\n",
    "print(z, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.21101],\n",
      "        [-0.56873]]) torch.Size([2, 1])\n",
      "tensor([[ 1.21101, -0.56873]]) torch.Size([1, 2])\n",
      "tensor([[ 1.21101, -0.56873]]) torch.Size([1, 2])\n",
      "tensor([[ 1.21101, -0.56873]])\n",
      "tensor([[ 1.21101, -0.56873]])\n"
     ]
    }
   ],
   "source": [
    "# 如何实现tensor维度之间的转置\n",
    "# tensor自身的t和transpose方法跟torch上的t和transpose方法功能类似\n",
    "x = torch.randn(2, 1)\n",
    "print(x, x.shape)\n",
    "y = torch.t(x)\n",
    "print(y, y.shape)\n",
    "z = torch.transpose(x, 1, 0)\n",
    "print(z, z.shape)\n",
    "print(x.t())\n",
    "print(x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.97537, 0.73018],\n",
      "         [0.34044, 0.98740]],\n",
      "\n",
      "        [[0.99114, 0.66789],\n",
      "         [0.17070, 0.74846]]])\n",
      "(tensor([[0.97537, 0.73018],\n",
      "        [0.99114, 0.66789]]), tensor([[0.34044, 0.98740],\n",
      "        [0.17070, 0.74846]]))\n"
     ]
    }
   ],
   "source": [
    "# 如何获取沿着某个维度切片后所有的切片\n",
    "# unbind删除某一维度之后，返回所有切片组成的元组列表\n",
    "x = torch.rand(2, 2, 2)\n",
    "print(x)\n",
    "print(torch.unbind(x, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 123\n",
      "state: tensor([123,   0,   0,   0,  ...,   0,   0,   0,   0], dtype=torch.uint8) 5056\n"
     ]
    }
   ],
   "source": [
    "# 手动设置随机种子\n",
    "torch.manual_seed(123)\n",
    "seed = torch.initial_seed()\n",
    "print(\"seed: {}\".format(seed))\n",
    "# 返回随机生成器状态\n",
    "state = torch.get_rng_state()\n",
    "print(\"state: {}\".format(state), len(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.36890, 0.01337, 0.59178],\n",
      "        [0.09264, 0.47245, 0.52203],\n",
      "        [0.60508, 0.53130, 0.94855]])\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 伯努利分布，结果只有0和1，第一个参数是概率p, 并且0<=p<=1\n",
    "torch.manual_seed(123)\n",
    "a = torch.rand(3, 3)\n",
    "print(a)\n",
    "b = torch.bernoulli(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 3])\n",
      "tensor([1, 0, 0, 0, 3, 0, 0, 0, 1, 0])\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 0],\n",
      "        [3, 3, 0, 0, 2, 0, 0, 3, 3, 0, 1, 3, 2, 3, 3]])\n",
      "tensor([[0, 1, 3, 2],\n",
      "        [3, 2, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# torch.multinomial的第一个参数为多项式权重，可以是向量，也可以是矩阵，由权重决定对“下标”的抽样\n",
    "# 为向量：replacement表示是否有放回的抽样，如果为True，结果行数为1，列数有num_samples指定；否则行数为1，列数<=权重weights长度\n",
    "weights1 = torch.Tensor([20, 10, 3, 2])\n",
    "a = torch.multinomial(weights1, num_samples=3, replacement=False)\n",
    "b = torch.multinomial(weights1, num_samples=10, replacement=True)\n",
    "# c = torch.multinomial(weights1, num_samples=5) !!!默认是不放回抽样，所以num_samples应该<=weights长度\n",
    "print(a)\n",
    "print(b)\n",
    "# print(c)\n",
    "# 为矩阵：replacement表示是否有放回的抽样，如果为True，结果行数为weights行数，列数由num_samples指定; \n",
    "# 否则行数为weights行数，列数<=权重weights每一行长度\n",
    "weights2 = torch.Tensor([[20, 10, 3, 2], [30, 4, 5, 60]])\n",
    "c = torch.multinomial(weights2, num_samples=15, replacement=True)\n",
    "d = torch.multinomial(weights2, num_samples=4, replacement=False)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.48034, 0.44650, 0.56860, 0.57181,  ..., 0.35206, 1.26491, 1.48555, 2.14253]) torch.Size([9000])\n",
      "tensor([0.00936, 0.29426, 0.80918, 1.27463, 0.18718, 1.13891, 0.50415, 1.32652, 0.71879]) torch.Size([9])\n",
      "tensor([ 0.11960,  0.52833, -0.29105,  0.60408,  1.43937,  0.51561,  1.04438,  0.85068,  0.45567]) torch.Size([9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 从正太分布中抽取随机数\n",
    "# 均值为0.5，方差为[0.10000, 0.20000, 0.30000, 0.40000, 0.50000, 0.60000, 0.70000, 0.80000, 0.90000]\n",
    "x = torch.normal(mean=0.5, std=torch.arange(0.1, 1, 0.0001))\n",
    "print(x, x.size())\n",
    "# 均值为[0.10000, 0.20000, 0.30000, 0.40000, 0.50000, 0.60000, 0.70000, 0.80000, 0.90000]，方差为0.5\n",
    "y = torch.normal(mean=torch.arange(0.1, 1, 0.1), std=0.5)\n",
    "print(y, y.size())\n",
    "# 均值为[0.10000, 0.20000, 0.30000, 0.40000, 0.50000, 0.60000, 0.70000, 0.80000, 0.90000]\n",
    "# 方差为[0.10000, 0.20000, 0.30000, 0.40000, 0.50000, 0.60000, 0.70000, 0.80000, 0.90000]\n",
    "z = torch.normal(mean=torch.arange(0.1, 1, 0.1), std=torch.arange(0.1, 1, 0.1))\n",
    "print(z, z.size())\n",
    "plt.plot(torch.arange(0.1, 1, 0.0001).data.numpy(), x.data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列化与反序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.11380,  1.04912,  0.25141],\n",
      "        [-0.54102,  1.24743, -0.68706]])\n",
      "tensor([[-1.11380,  1.04912,  0.25141],\n",
      "        [-0.54102,  1.24743, -0.68706]])\n"
     ]
    }
   ],
   "source": [
    "# 序列化模型\n",
    "x = torch.randn(2, 3)\n",
    "# 序列化torch.save方法\n",
    "torch.save(x, \"randn\")\n",
    "# 反序列化torch.load方法\n",
    "x_load = torch.load(\"randn\")\n",
    "print(x)\n",
    "print(x_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高并发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# PyTorch默认并发数等于计算机内核个数\n",
    "threads = torch.get_num_threads()\n",
    "print(threads)\n",
    "# 可通过set_num_threads方法设置并发数\n",
    "torch.set_num_threads(4)\n",
    "threads_1 = torch.get_num_threads()\n",
    "print(threads_1)\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元素级别的数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.00000, 0.50000, 0.12300, 0.40000, 0.50000, 0.99000])\n",
      "tensor([2.00000, 2.50000, 2.87700, 3.40000, 3.50000, 3.99000])\n",
      "tensor([0.54030, 0.87758, 0.99245, 0.92106, 0.87758, 0.54869])\n",
      "tensor([3.14159, 2.09440, 1.69411, 1.15928, 1.04720, 0.14154])\n"
     ]
    }
   ],
   "source": [
    "# 求元素绝对值\n",
    "a = torch.Tensor([-1, -0.5, -0.123, 0.4, 0.5, 0.99])\n",
    "print(torch.abs(a))\n",
    "# 每个元素加“n”\n",
    "print(torch.add(a, 3))\n",
    "# 余弦\n",
    "print(torch.cos(a))\n",
    "# 反余弦\n",
    "print(torch.acos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.80000,  0.40000,  0.07700,  0.60000,  0.60000,  1.29000])\n",
      "tensor([-0.20000,  0.40000,  0.67700,  2.20000,  5.40000,  1.29000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-b040d5b66a8a>:2: UserWarning: This overload of addcdiv is deprecated:\n",
      "\taddcdiv(Tensor input, Number value, Tensor tensor1, Tensor tensor2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcdiv(Tensor input, Tensor tensor1, Tensor tensor2, *, Number value, Tensor out) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  x = torch.addcdiv(a, 0.1, torch.Tensor([4, 9, 4, 6, 7, 3]), torch.Tensor([2, 1, 2, 3, 7, 1]))\n"
     ]
    }
   ],
   "source": [
    "# tensor相除再相加:a+0.1*tensor_a/tensor_b,返回新的结果; 需要注意的是a元素个数需要等于tensor_a/tensor_b元素个数\n",
    "x = torch.addcdiv(a, 0.1, torch.Tensor([4, 9, 4, 6, 7, 3]), torch.Tensor([2, 1, 2, 3, 7, 1]))\n",
    "print(x)\n",
    "# tensor相乘再相加:a+0.1*tensor_a*tensor_b,返回新的tensor; 需要注意的是a元素个数需要等于tensor_a*tensor_b元素个数\n",
    "y = torch.addcmul(a, 0.1, torch.Tensor([4, 9, 4, 6, 7, 3]), torch.Tensor([2, 1, 2, 3, 7, 1]))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9., -1., -8., -0.,  2.],\n",
      "        [ 2.,  4.,  5.,  9., -7.]])\n",
      "tensor([[-10.,  -2.,  -8.,  -1.,   1.],\n",
      "        [  1.,   3.,   4.,   8.,  -8.]])\n",
      "tensor([[-5.50000, -1.90000, -5.50000, -0.30000,  1.51000],\n",
      "        [ 1.20000,  3.80000,  4.01000,  5.50000, -5.50000]])\n",
      "tensor([[-0.91200, -0.19000, -0.80000, -0.03000,  0.15100],\n",
      "        [ 0.12000,  0.38000,  0.40100,  0.88800, -0.76000]])\n",
      "tensor([[ 9.12000,  1.90000,  8.00000,  0.30000, -1.51000],\n",
      "        [-1.20000, -3.80000, -4.01000, -8.88000,  7.60000]])\n",
      "tensor([[-0.10965, -0.52632, -0.12500, -3.33333,  0.66225],\n",
      "        [ 0.83333,  0.26316,  0.24938,  0.11261, -0.13158]])\n",
      "tensor([0.81379, 0.91287, 0.51299, 0.49938, 0.33558])\n",
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# 向上取整\n",
    "x = torch.Tensor([[-9.12, -1.9, -8, -0.3, 1.51], [1.2, 3.8, 4.01, 8.88, -7.6]])\n",
    "print(torch.ceil(x))\n",
    "# 向下取整\n",
    "print(torch.floor(x))\n",
    "# 夹逼函数，将每个元素限制在给定区间范围内,小于范围下限的被强制设置为下限值；大于上限的被强制设置为上限值\n",
    "print(torch.clamp(x, -5.5, 5.5))\n",
    "# 乘法\n",
    "print(torch.mul(x, 0.1))\n",
    "# 取相反数\n",
    "print(torch.neg(x))\n",
    "# 取倒数\n",
    "print(torch.reciprocal(x))\n",
    "# 取平方根倒数：每个元素的平方根倒数\n",
    "print(torch.rsqrt(x[x>0]))\n",
    "# 求平方根\n",
    "print(torch.sqrt(torch.Tensor([4, 16])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.33333, 3.00000])\n",
      "tensor([3., 3.])\n",
      "tensor([0.10000, 0.30000, 1.00000, 0.00000, 1.00000])\n",
      "tensor([0.10000, 0.30000, 1.00000, 0.00000, 1.00000])\n",
      "tensor([0.10000, 0.30000, 0.00000, 0.00000, 0.00000])\n",
      "tensor([1., 3.])\n",
      "tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "# 除法\n",
    "x = torch.Tensor([4, 9])\n",
    "y = torch.div(x, 3)\n",
    "print(y)\n",
    "z = torch.div(x, y)\n",
    "print(z)\n",
    "# 计算除法余数的fmod和remainder方法,相当于python中的\"%\"算子\n",
    "q = torch.Tensor([2.1, 2.3, 5, 6, 7])\n",
    "print(torch.fmod(q, 2))\n",
    "print(torch.remainder(q, 2))\n",
    "# 返回浮点数的小数部分\n",
    "print(torch.frac(q))\n",
    "# 四舍五入\n",
    "print(torch.round(y))\n",
    "# 指数运算\n",
    "print(torch.exp(torch.Tensor([0, math.log(2)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([1.31326, 2.12693])\n",
      "tensor([1.44270, 2.88539])\n",
      "tensor([0.43429, 0.86859])\n",
      "tensor([2.71828, 7.38906]) tensor([ 7.38906, 54.59815])\n"
     ]
    }
   ],
   "source": [
    "# 自然对数\n",
    "x = torch.Tensor([math.e, math.e**2])\n",
    "print(torch.log(x))\n",
    "# 对输入x加1平滑处理后再求log\n",
    "print(torch.log1p(x))\n",
    "# 2为底的对数\n",
    "print(torch.log2(x))\n",
    "# 10为底的对数\n",
    "print(torch.log10(x))\n",
    "# 幂运算\n",
    "print(torch.pow(x, 1), torch.pow(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10.])\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([5.00000, 5.50000, 6.00000, 6.50000, 7.00000, 7.50000, 8.00000, 8.50000, 9.00000, 9.50000])\n"
     ]
    }
   ],
   "source": [
    "# 线性插值：outi=starti+weight∗(endi−starti)\n",
    "# 带`_`线的方法为in-place类型算子，不会创建新的tensor而是改变原tensor的值\n",
    "x = torch.zeros(10).fill_(10)\n",
    "print(x)\n",
    "y = torch.arange(10).double()\n",
    "print(y)\n",
    "z = torch.lerp(x, y, 0.5)  # out[i] = y[i] + weight * (x[i] - y[i])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.], dtype=torch.float32)\n",
      "tensor([0.00669, 0.01799, 0.04743, 0.11920, 0.26894, 0.50000, 0.73106, 0.88080, 0.95257, 0.98201],\n",
      "       dtype=torch.float32)\n",
      "tensor([-1., -1., -1., -1., -1.,  0.,  1.,  1.,  1.,  1.], dtype=torch.float32)\n",
      "tensor([-0., -1., -1.,  0.,  2.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# 求每个元素的sigmod值\n",
    "# Sigmoid的计算公式为1 / (1 + math.e^(-x))\n",
    "# Sigmod值位于[0,1],可视为概率值，在激活函数中应用较广\n",
    "x = torch.arange(-5, 5, 1).float()\n",
    "print(x)\n",
    "print(torch.sigmoid(x))\n",
    "# 符号函数，根据元素的正负，返回+1和-1,元素0返回0\n",
    "print(torch.sign(x))\n",
    "# 截断值（标量x的截断值是最接近其整数的，其比x更接近零。简单理解就是截取小数点前面的数）\n",
    "print(torch.trunc(torch.Tensor([-0.9, -1.2, -1.9, 0, 2.1, 2.7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规约计算（一般是指分组聚合计算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3., 4., 5., 6.],\n",
      "        [9., 8., 7., 6., 5.]])\n",
      "tensor([[2.00000e+00, 6.00000e+00, 2.40000e+01, 1.20000e+02, 7.20000e+02],\n",
      "        [9.00000e+00, 7.20000e+01, 5.04000e+02, 3.02400e+03, 1.51200e+04]])\n",
      "tensor([[ 2.,  3.,  4.,  5.,  6.],\n",
      "        [18., 24., 28., 30., 30.]])\n",
      "tensor([[ 2.,  5.,  9., 14., 20.],\n",
      "        [ 9., 17., 24., 30., 35.]])\n",
      "tensor([[ 2.,  3.,  4.,  5.,  6.],\n",
      "        [11., 11., 11., 11., 11.]])\n",
      "tensor([  720., 15120.])\n",
      "tensor([18., 24., 28., 30., 30.])\n",
      "tensor(55.)\n",
      "tensor([20., 35.])\n"
     ]
    }
   ],
   "source": [
    "# 计算累积(Cumulative), 可以通过dim指定沿着某个维度计算累积\n",
    "x = torch.Tensor([[2, 3, 4, 5, 6], [9, 8, 7, 6, 5]])\n",
    "print(x)\n",
    "print(torch.cumprod(x, dim=1))\n",
    "print(torch.cumprod(x, dim=0))\n",
    "# 计算累和\n",
    "print(torch.cumsum(x, dim=1))\n",
    "print(torch.cumsum(x, dim=0))\n",
    "# 计算所有元素的乘积\n",
    "print(torch.prod(x, dim=1))\n",
    "print(torch.prod(x, dim=0))\n",
    "# 计算所有元素的和\n",
    "print(torch.sum(x))\n",
    "print(torch.sum(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(8.)\n",
      "tensor(5.83095)\n",
      "tensor(5.)\n",
      "tensor([20., 35.])\n",
      "tensor([11., 11., 11., 11., 11.])\n",
      "tensor([ 9.48683, 15.96872])\n",
      "tensor([9.21954, 8.54400, 8.06226, 7.81025, 7.81025])\n"
     ]
    }
   ],
   "source": [
    "# 距离公式，常用于模型损失值的计算。计算采用p-norm范数。p=1为曼哈顿距离；p=2为欧氏距离,默认为计算欧式距离\n",
    "x = torch.Tensor([[2, 3, 4, 5, 6], [9, 8, 7, 6, 5]])\n",
    "y = torch.Tensor([[2, 3, 4, 5, 6], [4, 5, 7, 6, 5]])\n",
    "print(torch.dist(x, y, p=0))\n",
    "print(torch.dist(x, y, p=1))\n",
    "print(torch.dist(x, y, p=2))\n",
    "print(torch.dist(x, y, np.inf))\n",
    "# p-norm范数\n",
    "print(torch.norm(x, p=1, dim=1))\n",
    "print(torch.norm(x, p=1, dim=0))\n",
    "print(torch.norm(x, p=2, dim=1))\n",
    "print(torch.norm(x, p=2, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.50000)\n",
      "tensor([4., 7.])\n",
      "tensor([5.50000, 5.50000, 5.50000, 5.50000, 5.50000])\n",
      "tensor(5.)\n",
      "torch.return_types.median(\n",
      "values=tensor([4., 7.]),\n",
      "indices=tensor([2, 2]))\n",
      "torch.return_types.mode(\n",
      "values=tensor([2., 5.]),\n",
      "indices=tensor([0, 4]))\n",
      "tensor(4.72222)\n",
      "tensor([24.50000, 12.50000,  4.50000,  0.50000,  0.50000])\n",
      "tensor(2.17307)\n",
      "tensor([4.94975, 3.53553, 2.12132, 0.70711, 0.70711])\n"
     ]
    }
   ],
   "source": [
    "# 均值·中位数·众数·方差·标准差\n",
    "# 均值\n",
    "x = torch.Tensor([[2, 3, 4, 5, 6], [9, 8, 7, 6, 5]])\n",
    "print(torch.mean(x))\n",
    "print(torch.mean(x, dim=1))\n",
    "print(torch.mean(x, dim=0))\n",
    "# 中位数，指定dim将返回两个tensor，第一个是中位数，第二个tensor为index索引\n",
    "print(torch.median(x))\n",
    "print(torch.median(x, dim=1))\n",
    "# 众数\n",
    "print(torch.mode(x))\n",
    "# 方差(计算的实际上是样本方差即: torch.square(x-x.mean()).sum() / (n-1) )\n",
    "print(torch.var(x))\n",
    "print(torch.var(x, 0))\n",
    "# 标准差\n",
    "print(torch.std(x))\n",
    "print(torch.std(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值比较运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False,  True],\n",
      "        [ True, False,  True]])\n",
      "True\n",
      "False\n",
      "tensor([[ True, False,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False,  True, False],\n",
      "        [False,  True, False]])\n",
      "tensor([[False,  True, False],\n",
      "        [False,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "# 元素相等比较，相等返回1，不相等返回0\n",
    "x = torch.Tensor([[2, 3, 5], [4, 7, 9]])\n",
    "y = torch.Tensor([[2, 4, 5], [4, 8, 9]])\n",
    "z = torch.Tensor([[2, 3, 5], [4, 7, 9]])\n",
    "print(torch.eq(x, y))\n",
    "# 比较两个Tensor是否相等\n",
    "print(torch.equal(x, z))\n",
    "print(torch.equal(x, y))\n",
    "# 逐一比较tensor1中元素是否大于等于tensor2中元素\n",
    "print(torch.ge(x, y))\n",
    "# 逐一比较tensor1中元素是否大于tensor2中元素\n",
    "print(torch.gt(x, y))\n",
    "# 逐一比较tensor1中元素是否小于等于tensor2中的元素\n",
    "print(torch.le(x, y))\n",
    "# 逐一比较tensor1中元素是否小于tensor2中元素\n",
    "print(torch.lt(x, y))\n",
    "# 逐一比较两个tensor中的元素是否不相等\n",
    "print(torch.ne(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n",
      "torch.return_types.max(\n",
      "values=tensor([5., 9.]),\n",
      "indices=tensor([2, 2]))\n",
      "tensor(2.)\n",
      "torch.return_types.min(\n",
      "values=tensor([2., 3., 5.]),\n",
      "indices=tensor([0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 最大值·最小值\n",
    "x = torch.Tensor([[2, 3, 5], [4, 7, 9]])\n",
    "print(torch.max(x))\n",
    "# 若指定了dim，返回两个tensor，第一个tensor为指定维度上的最大值；第二个tensor为指定维度上对应最大值所在的索引\n",
    "print(torch.max(x, dim=1))\n",
    "# 最小值\n",
    "print(torch.min(x))\n",
    "print(torch.min(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([[ 3.,  5., 20.],\n",
      "        [ 4.,  9., 70.]]),\n",
      "indices=tensor([[1, 2, 0],\n",
      "        [0, 2, 1]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[ 4.,  3.,  5.],\n",
      "        [20., 70.,  9.]]),\n",
      "indices=tensor([[1, 0, 0],\n",
      "        [0, 1, 1]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[20., 70.,  9.],\n",
      "        [ 4.,  3.,  5.]]),\n",
      "indices=tensor([[0, 1, 1],\n",
      "        [1, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "# 排序\n",
    "x = torch.Tensor([[20, 3, 5], [4, 70, 9]])\n",
    "# 不指定dim，则默认按shape(-1)最后维度所在的方向进行升序排列；返回值第二个tensor为排序索引组成的tensor\n",
    "print(torch.sort(x))\n",
    "print(torch.sort(x, dim=0))\n",
    "# 指定descending关键字参数，设定升序还是降序\n",
    "print(torch.sort(x, dim=0, descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[20.,  5.],\n",
      "        [70.,  9.]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [1, 2]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[3., 5.],\n",
      "        [4., 9.]]),\n",
      "indices=tensor([[1, 2],\n",
      "        [0, 2]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[20., 70.,  9.],\n",
      "        [ 4.,  3.,  5.]]),\n",
      "indices=tensor([[0, 1, 1],\n",
      "        [1, 0, 0]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[20.,  5.],\n",
      "        [70.,  9.]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [1, 2]]))\n"
     ]
    }
   ],
   "source": [
    "# topK选择最大的或者最小的K个元素作为返回值\n",
    "x = torch.Tensor([[20, 3, 5], [4, 70, 9]])\n",
    "# k关键字参数指定返回最大或最小的几个元素,默认取最大的元素\n",
    "print(torch.topk(x, k=2))\n",
    "# largest设置为false表示取最小的topk值\n",
    "print(torch.topk(x, k=2, largest=False))\n",
    "# 指定dim关键字参数则表示沿着dim维度所在的方向取topk\n",
    "print(torch.topk(x, k=2, dim=0))\n",
    "print(torch.topk(x, k=2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21., 15., 24., 22., 17., 20., 17., 27., 20., 17.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 直方图操作\n",
    "# 计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中\n",
    "# 如果min和max都为0, 则利用数据中的最大最小值作为边界\n",
    "x = torch.rand(200)\n",
    "y = torch.histc(x, min=0, max=1, bins=10)\n",
    "print(y)\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.97456, 0.90584])\n",
      "tensor([[0.97456, 0.00000],\n",
      "        [0.00000, 0.90584]])\n",
      "tensor([[0.00000, 0.00000, 0.97456, 0.00000],\n",
      "        [0.00000, 0.00000, 0.00000, 0.90584],\n",
      "        [0.00000, 0.00000, 0.00000, 0.00000],\n",
      "        [0.00000, 0.00000, 0.00000, 0.00000]])\n",
      "tensor([[0.00000, 0.00000, 0.00000],\n",
      "        [0.97456, 0.00000, 0.00000],\n",
      "        [0.00000, 0.90584, 0.00000]])\n"
     ]
    }
   ],
   "source": [
    "# 对角矩阵\n",
    "a = torch.rand(2)\n",
    "print(a)\n",
    "# diag设置对角矩阵，diagonal等于0，设置主对角线\n",
    "x = torch.diag(a, diagonal=0)\n",
    "print(x)\n",
    "# diagonal大于0，设置主对角线之上diagonal对应位置的值\n",
    "x = torch.diag(a, diagonal=2)\n",
    "print(x)\n",
    "# diagonal小于0，设置主对角线之下diagonal对应位置的值\n",
    "x = torch.diag(a, diagonal=-1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.74642, -0.00769,  0.18950,  0.30077],\n",
      "        [ 0.33682, -0.10038, -1.25118,  1.92749]])\n",
      "tensor(-0.84680)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵的迹\n",
    "# 二维矩阵主对角线上元素之和\n",
    "x = torch.randn(2, 4)\n",
    "print(x)\n",
    "print(torch.trace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.29483, 0.00000, 0.00000, 0.00000, 0.00000],\n",
      "        [0.59429, 0.77377, 0.00000, 0.00000, 0.00000]])\n",
      "tensor([[ 2.29483, -0.24912,  0.41962,  0.00000,  0.00000],\n",
      "        [ 0.59429,  0.77377, -1.08128, -0.68329,  0.00000]])\n",
      "tensor([[0.00000, 0.00000, 0.00000, 0.00000, 0.00000],\n",
      "        [0.59429, 0.00000, 0.00000, 0.00000, 0.00000]])\n",
      "tensor([[ 2.29483, -0.24912,  0.41962,  1.26282,  1.26117],\n",
      "        [ 0.00000,  0.77377, -1.08128, -0.68329,  0.04989]])\n",
      "tensor([[0.00000, 0.00000, 0.00000, 0.00000, 1.26117],\n",
      "        [0.00000, 0.00000, 0.00000, 0.00000, 0.00000]])\n",
      "tensor([[ 2.29483, -0.24912,  0.41962,  1.26282,  1.26117],\n",
      "        [ 0.59429,  0.77377, -1.08128, -0.68329,  0.04989]])\n"
     ]
    }
   ],
   "source": [
    "# 下三角矩阵\n",
    "# 参数diagonal控制对角线: diagonal = 0, 主对角线; diagonal > 0, 主对角线之上; diagonal < 0, 主对角线之下\n",
    "x = torch.randn(2, 5)\n",
    "print(torch.tril(x))\n",
    "print(torch.tril(x, diagonal=2))\n",
    "print(torch.tril(x, diagonal=-1))\n",
    "# 上三角矩阵\n",
    "# 参数diagonal控制对角线: diagonal = 0, 主对角线; diagonal > 0, 主对角线之上; diagonal < 0, 主对角线之下\n",
    "print(torch.triu(x))\n",
    "print(torch.triu(x, diagonal=4))\n",
    "print(torch.triu(x, diagonal=-1))  # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([1, 3, 1])\n",
      "tensor([[[ 46.],\n",
      "         [118.]]]) tensor([ 46., 118.])\n",
      "tensor([[ 46.],\n",
      "        [118.]])\n",
      "tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "# bmm矩阵乘积\n",
    "# 矩阵A的列数需等于矩阵B的行数\n",
    "# 1*2*3\n",
    "x = torch.Tensor([[[1, 2, 3], [4, 5, 6]]])\n",
    "print(x.shape)\n",
    "# 1*3*1\n",
    "y = torch.Tensor([[[9], [8], [7]]])\n",
    "print(y.shape)\n",
    "# res 1*2*1\n",
    "print(torch.bmm(x, y), torch.bmm(x, y).squeeze(0).squeeze(1))\n",
    "# 注意和mm的区别，bmm为batch matrix multiplication，而mm为matrix multiplication\n",
    "print(torch.mm(x.squeeze(0), y.squeeze(0)))\n",
    "# 计算两个一维张量的点积torch.dot，两个向量对应位置的元素相乘再相加\n",
    "x = torch.Tensor([1, 2, 3, 4])\n",
    "y = torch.Tensor([4, 3, 2, 1])\n",
    "print(torch.dot(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 2])\n",
      "tensor([[110.10000, 140.20000]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵相乘再相加\n",
    "# addmm方法用于两个矩阵相乘结果再加到M矩阵，用beta调节M矩阵权重；用alpha调节矩阵乘积结果系数\n",
    "# 两个相乘的矩阵维度为2，分别表示[width，length]，第一个矩阵的length应该等于第二个矩阵的width满足矩阵相乘条件\n",
    "# out = (beta*M) + (alpha*mat1·mat2)\n",
    "x = torch.Tensor([[1, 2]])\n",
    "print(x.shape)\n",
    "# batch1: 1*1*3\n",
    "batch1 = torch.Tensor([[1, 2, 3]])\n",
    "print(batch1.shape)\n",
    "# batch2: 1*3*2\n",
    "batch2 = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(batch2.shape)\n",
    "print(torch.addmm(x, batch1, batch2, beta=0.1, alpha=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1, 3])\n",
      "torch.Size([1, 3, 2])\n",
      "tensor([[220.10000, 280.20000]])\n"
     ]
    }
   ],
   "source": [
    "# 批矩阵相乘再相加\n",
    "# addbmm方法用于批矩阵相乘结果再加到M矩阵，用beta调节M矩阵权重；用alpha调节矩阵乘积结果系数\n",
    "# 两个相乘的矩阵维度为3，分别表示[batchsize,width，length]，第一个矩阵的length应该等于第二个矩阵的width满足矩阵相乘条件\n",
    "# 两个相乘矩阵batchsize应该相等\n",
    "# res = (beta*M) + (alpha*sum(batch1i·batch2i, i=0, b))\n",
    "x = torch.Tensor([[1, 2]])\n",
    "print(x.shape)\n",
    "# batch1: 1*1*3\n",
    "batch1 = torch.Tensor([[[1, 2, 3]]])\n",
    "print(batch1.shape)\n",
    "# batch2: 1*3*2\n",
    "batch2 = torch.Tensor([[[1, 2], [3, 4], [5, 6]]])\n",
    "print(batch2.shape)\n",
    "print(torch.addbmm(x, batch1, batch2, beta=0.1, alpha=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.bmm(batch1, batch2)\n",
    "print(c.shape)\n",
    "sum(c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([1])\n",
      "tensor([130., 260., 390.])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘向量再相加\n",
    "# addmv方法用于矩阵和向量相乘结果再加到M矩阵，用beta调节M矩阵权重；用alpha调节矩阵同向量乘积结果\n",
    "# 矩阵的列应该等于向量长度以满足相乘条件\n",
    "# out = (beta*tensor) + (alpha*(mat·vec))\n",
    "# 1*3\n",
    "x = torch.Tensor([1, 2, 3])\n",
    "print(x.shape)\n",
    "# batch1: 1*3\n",
    "mat = torch.Tensor([[1], [2], [3]])\n",
    "print(mat.shape)\n",
    "# batch2:\n",
    "vec = torch.Tensor([3])\n",
    "print(vec.shape)\n",
    "print(torch.addmv(x, mat, vec, beta=100, alpha=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mv(mat, vec))\n",
    "print(torch.mv(mat, vec).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[19.02208,  0.00000],\n",
      "        [ 6.18709,  0.00000],\n",
      "        [-2.20917,  0.00000]]),\n",
      "eigenvectors=tensor([]))\n",
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[19.02208,  0.00000],\n",
      "        [ 6.18709,  0.00000],\n",
      "        [-2.20917,  0.00000]]),\n",
      "eigenvectors=tensor([[ 0.33846,  0.78463, -0.05279],\n",
      "        [ 0.53729, -0.42130, -0.72857],\n",
      "        [ 0.77251, -0.45483,  0.68294]]))\n"
     ]
    }
   ],
   "source": [
    "# 计算方阵的特征值和特征向量(eigenvector)\n",
    "# eigenvectors(bool)如果为True，则同时计算特征值和特征向量，否则只计算特征值\n",
    "x = torch.Tensor([[9, 2, 3], [4, 5, 8], [7, 10, 9]])\n",
    "print(torch.eig(x))\n",
    "print(torch.eig(x, eigenvectors=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 每个维度的大小都相等，不需要广播即可参与正常的运算\n",
    "x = torch.ones(1, 2, 3)\n",
    "y = torch.zeros(1, 2, 3)\n",
    "print((x+y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.],\n",
      "        [7.]])\n"
     ]
    }
   ],
   "source": [
    "# 每个tensor至少有一个维度\n",
    "x = torch.Tensor([2])\n",
    "y = torch.Tensor([[3], [5]])\n",
    "# x将自动扩展为[[2], [2]]，然后参与计算\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5, 6, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-79cac8e4a59e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "# 满足tensor的“维度三条件”\n",
    "# 从最里面的维度开始遍历，要么维度大小相等，要么其中一个维度大小为1，要么其中一个维度缺失，三者满足其一便可自动广播\n",
    "# 1楼：7==7相等\n",
    "# 2楼：其中一个size==1\n",
    "# 3楼：其中一个size==1\n",
    "# 4楼：4==4相等\n",
    "# 5楼：3==3相等\n",
    "# 6楼：其中一个Tensor维度缺失\n",
    "x = torch.randn(2, 3, 4, 5, 6, 7)\n",
    "y = torch.rand(3, 4, 1, 1, 7)\n",
    "print((x+y).shape)\n",
    "# 下面的Tensor不能广播，因此不能正常参与运算\n",
    "a = torch.randn(2, 3, 4, 5, 6, 7)\n",
    "b = torch.rand(3, 4, 1, 2, 7)\n",
    "print((a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播计算规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2*3\n",
    "x = torch.randn(2, 3)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2, 3])\n",
      "torch.Size([1, 4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1*4*2*3\n",
    "y = torch.rand(1, 4, 2, 3)\n",
    "print(y.shape)\n",
    "# x维度为2，y的维度为4，因此会在x外增加大小为1的维度，变成[[[[1, 2, 3], [1, 2, 3]]]]\n",
    "# 维度相等后对应维度按照“广播规则”进行广播，使对应维度大小相等\n",
    "# 输出结果：维度等于高维度Tensor的维度，每个维度大小取两个Tensor对应维度的最大值\n",
    "print((x+y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU设备及并行编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Device: cpu\n",
      "Device Type: cpu\n",
      "CPU Device: 【cpu:0】\n",
      "GPU Device: 【cuda:0】\n",
      "GPU Device: 【cuda:0】\n",
      "Total GPU Count: 1\n",
      "Total CPU Count: 16\n",
      "GeForce RTX 2080 Ti\n",
      "GPU Is Available: True\n"
     ]
    }
   ],
   "source": [
    "# 使用torch.device指定设备，PyTorch默认使用CPU设备\n",
    "print(\"Default Device: {}\".format(torch.Tensor([4, 5, 6]).device))\n",
    "\n",
    "# CPU设备可以使用\"cpu:0\"来指定\n",
    "device = torch.Tensor([1, 2, 3], device=\"cpu:0\").device\n",
    "print(\"Device Type: {}\".format(device))\n",
    "\n",
    "# 用torch.device指定cpu:0设备\n",
    "cpu1 = torch.device(\"cpu:0\")\n",
    "print(\"CPU Device: 【{}:{}】\".format(cpu1.type, cpu1.index))\n",
    "\n",
    "# 使用索引的方式，默认使用CUDA设备\n",
    "gpu = torch.device(0)\n",
    "print(\"GPU Device: 【{}:{}】\".format(gpu.type, gpu.index))\n",
    "\n",
    "# 也可以通过torch.device(\"cuda:0\")的方式指定使用哪块GPU\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "print(\"GPU Device: 【{}:{}】\".format(gpu.type, gpu.index))\n",
    "\n",
    "# 查看所有可用的GPU设备的个数\n",
    "print(\"Total GPU Count: {}\".format(torch.cuda.device_count()))\n",
    "\n",
    "# 获取系统CPU设备的数量\n",
    "print(\"Total CPU Count: {}\".format(torch.cuda.os.cpu_count()))\n",
    "\n",
    "# 获取GPU设备的名称\n",
    "print(torch.cuda.get_device_name(torch.device(\"cuda:0\")))\n",
    "\n",
    "# GPU设备是否可用\n",
    "print(\"GPU Is Available: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将Tensor从CPU转移到GPU并进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "cuda:0\n",
      "tensor([[1., 4., 7.],\n",
      "        [3., 6., 9.],\n",
      "        [2., 5., 8.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor方法默认使用CPU设备\n",
    "cpu_tensor = torch.Tensor([[1, 4, 7], [3, 6, 9], [2, 5, 8]])\n",
    "print(cpu_tensor.device)\n",
    "\n",
    "# 使用to()方法将cup_tensor转到GPU上\n",
    "gpu_tensor1 = cpu_tensor.to(torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor1.device)\n",
    "\n",
    "# 使用cuda方法将cpu_tensor转移到GPU设备上\n",
    "gpu_tensor2 = cpu_tensor.cuda(torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor2.device)\n",
    "\n",
    "# 使用copy_方法将cpu_tensor转移到GPU设备上\n",
    "gpu_tensor3 = torch.ones((3, 3), device=torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor3)\n",
    "gpu_tensor3.copy_(cpu_tensor)\n",
    "print(gpu_tensor3.device)\n",
    "print(gpu_tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接在GPU设备上创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n",
      "tensor([[0.40233, 0.39581, 0.71156, 0.44878],\n",
      "        [0.51772, 0.19462, 0.24883, 0.06506],\n",
      "        [0.29673, 0.00097, 0.54877, 0.14315]], device='cuda:0')\n",
      "tensor([[-0.88482,  0.70004, -0.72039,  0.15135,  1.46833],\n",
      "        [-0.36963,  0.74118, -0.87507,  0.16469,  2.32766]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 使用torch.tensor方法，需要注意和torch.Tensor的区别，前者可通过device指定GPU设备，而后者只能在CPU设备上创建\n",
    "gpu_tensor1 = torch.tensor([[2, 5, 8], [1, 4, 7], [3, 6, 9]], device=torch.device(\"cuda:0\"))\n",
    "print(gpu_tensor1.device)\n",
    "\n",
    "# torch.Tensor方法的device只能是CPU设备，否则报错。在CPU设备上创建后可以使用copy_、cuda、to等方法转移到GPU设备上\n",
    "cpu_tensor = torch.Tensor([[2, 5, 8], [1, 4, 7], [3, 6, 9]], device=torch.device(\"cpu:0\"))\n",
    "print(cpu_tensor.device)\n",
    "\n",
    "# 除了torch.tensor方法，torch上的其他（如rand、randn等）方法也可以通过device参数指定GPU设备\n",
    "print(torch.rand((3, 4), device=torch.device(\"cuda:0\")))\n",
    "print(torch.randn((2, 5), device=torch.device(\"cuda:0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Stream:<torch.cuda.Stream device=cuda:0 cuda_stream=0x0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x7b62d8f0>\n"
     ]
    }
   ],
   "source": [
    "### 一个错误的示例\n",
    "cuda = torch.device(\"cuda:0\")\n",
    "# 创建新的Stream\n",
    "s = torch.cuda.Stream()\n",
    "# 使用默认Stream\n",
    "A = torch.randn((1, 10), device=cuda)\n",
    "print(\"Current Stream:{}\".format(torch.cuda.current_stream()))\n",
    "for i in range(10):\n",
    "    # 在新的Stream上对默认Stream上创建的Tensor求和\n",
    "    with torch.cuda.stream(s):\n",
    "        print(\"Current Stream: {}\".format(torch.cuda.current_stream()))\n",
    "        # sum() may start execution before randn() finishes!\n",
    "        B = torch.sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaul Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x5b8dfc0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x5b8dfc0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x5b8dfc0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x5b8dfc0>\n",
      "Current Stream: <torch.cuda.Stream device=cuda:0 cuda_stream=0x5b8dfc0>\n"
     ]
    }
   ],
   "source": [
    "# 使用Synchronize同步方法纠正上面的错误代码\n",
    "s = torch.cuda.Stream()\n",
    "A = torch.randn((1000, 700), device=cuda)\n",
    "default_stream = torch.cuda.current_stream()\n",
    "print(\"Defaul Stream: {}\".format(default_stream))\n",
    "# 等待创建A的Stream完成\n",
    "torch.cuda.Stream.synchronize(default_stream)\n",
    "# 在新的Stream上对默认Stream上创建的Tensor求和\n",
    "with torch.cuda.stream(s):\n",
    "    for i in range(5):\n",
    "        print(\"Current Stream: {}\".format(torch.cuda.current_stream()))\n",
    "        B = torch.sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 释放没有使用的缓存数据\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25165824"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取缓存数据大小\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27262976"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取最大缓存大小\n",
    "torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5604352"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大分配内存大小\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 140527432742272\n",
      "id: 140527759524480\n",
      "id: 140527432946304\n",
      "cuda:0\n",
      "tensor([[ 1.,  2.,  4.],\n",
      "        [ 5.,  7.,  9.],\n",
      "        [ 3.,  7., 10.]], device='cuda:0')\n",
      "is_pinned : False/True\n"
     ]
    }
   ],
   "source": [
    "# 通过调用torch.pin_memory方法将Tensor复制到固定缓冲区\n",
    "x = torch.Tensor([[1, 2, 4], [5, 7, 9], [3, 7, 10]])\n",
    "y = torch.pinverse(x)\n",
    "# 或在Tensor上直接调用pin_memory方法\n",
    "z = x.pin_memory()\n",
    "# 需要注意的是，pin_memory方法返回Tensor对象的拷贝，因此内存地质是不一样的\n",
    "print(\"id: {}\".format(id(x)))\n",
    "print(\"id: {}\".format(id(y)))\n",
    "print(\"id: {}\".format(id(z)))\n",
    "# 一旦将Tensor放入固定缓冲区，就可以使用asynchrinize方式将数据复制到GPU设备上，\n",
    "# 如使用cuda方法，将关键字参数non_blocking设置为True\n",
    "a = z.cuda(non_blocking=True)\n",
    "print(a.device)\n",
    "print(a)\n",
    "print(\"is_pinned : {}/{}\".format(x.is_pinned(), z.is_pinned()))  # 判断Tensor是否被复制到固定缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 在数据加载器Dataloader中指定pin_memory关键字参数为True\n",
    "# 创建Dataset实例\n",
    "mnist_dataset = datasets.MNIST('./pytorch_dataset/', train=True, download=True, \n",
    "                               transform=Compose([ToTensor(), Normalize((0.1,), (0.3,))]))\n",
    "data_loader = DataLoader(mnist_dataset, batch_size=32, shuffle=True, num_workers=10, **{\"pin_memory\": True})\n",
    "\n",
    "# Dataloader是可以迭代的，使用for循环便可以不断输出数据，使用to方法可以将迭代产生的Tensor发送到GPU设备上\n",
    "for data, indx in data_loader:\n",
    "    # print(data[0].is_pinned(), indx.is_pinned())\n",
    "    # 将数据复制到GPU设备上\n",
    "    data.to(torch.device(\"cuda:0\"))\n",
    "    indx.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动设备感知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 创建默认的CPU设备\n",
    "device = torch.device(\"cpu:0\")\n",
    "\n",
    "# 如果GPU设备可用，则将默认设备改为GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 使用torch.tensor方法、关键字参数device指定设备\n",
    "a = torch.tensor([1, 2, 3], device=device)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.59592]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.24814], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "regression = LinearRegression().to(device=device)\n",
    "for para in regression.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并发编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10708\n",
      "10712\n",
      "10718\n",
      "10724\n",
      "10730\n",
      "10736\n",
      "10742\n",
      "10748\n",
      "my pid is: 10708\n",
      "my pid is: 10712\n",
      "my pid is: 10718\n",
      "my pid is: 10724\n",
      "my pid is: 10730\n",
      "my pid is: 10736\n",
      "my pid is: 10742\n",
      "my pid is: 10748\n",
      "my pid is: 10754\n",
      "10754\n",
      "10760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "def foo(q):  # 将队列对象传递给函数\n",
    "    pid = os.getpid()\n",
    "    q.put('my pid is: {}'.format(pid))\n",
    "    print(pid)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # 设置启动进程方式，Windows环境中默认为spawn，Linux环境中默认为fork\n",
    "    # mp.set_start_method('spawn')\n",
    "    # 创建队列对象\n",
    "    q = mp.Queue()\n",
    "    ps = []\n",
    "    # 创建10个进程，传递运行函数和参数\n",
    "    [ps.append(mp.Process(target=foo, args=(q,))) for i in range(10)]\n",
    "    # 启动进程\n",
    "    [p.start() for p in ps]\n",
    "    # join()方法可以使主线程阻塞，等待子线程执行完成再执行\n",
    "    [p.join for p in ps]\n",
    "    # 获取队列数据\n",
    "    data = q.get()\n",
    "    while data:\n",
    "        print(data)\n",
    "        data = q.get()\n",
    "        if q.empty():\n",
    "            print(data)\n",
    "            data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),0.001)        \n",
    "    #开始训练\n",
    "    num_epochs = 1000\n",
    "    for i in range(num_epochs):\n",
    "        input_data = x_train.unsqueeze(1)\n",
    "        target = y_train.unsqueeze(1)\n",
    "        out = model(input_data)\n",
    "        loss = criterion(out,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"PID: {}, Epoch: [{}/{}], loss: [{:.4f}]\".format(os.getpid(), i+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(20)\n",
    "y = np.array([5*x[i]+random.randint(1, 20) for i in range(len(x))])\n",
    "x_train = torch.from_numpy(x).double()\n",
    "y_train = torch.from_numpy(y).double()\n",
    "# 新建模型、误差函数和优化器\n",
    "model = LinearRegression()\n",
    "model.share_memory()  # 训练出的权重参数需要多进程共享\n",
    "processes = []\n",
    "for rank in range(10):\n",
    "    p = mp.Process(target=train, args=(x_train, y_train, model))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# 预测一波\n",
    "input_data = x_train.unsqueeze(1)\n",
    "predict = model(input_data)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.plot(x_train.data.numpy(), predict.squeeze(1).data.numpy(), \"r\")\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = datasets.MNIST('./pytorch_dataset/', train=True, download=True, \n",
    "                               transform=Compose([ToTensor(), Normalize((0.0,), (1.0,))]))\n",
    "\n",
    "data_loader = DataLoader(mnist_dataset, batch_size=32, shuffle=True, num_workers=10, **{\"pin_memory\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 50)\n",
    "        self.layer2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入层到隐藏层，使用tanh激活函数\n",
    "        x = self.layer1(x.reshape(-1, 784))\n",
    "        x = torch.tanh(x)\n",
    "        # 隐藏层到输出层，使用ReLU激活函数\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        # 使用F.log_softmax激活函数，最后计算损失值时要使用NLLLoss负对数似然损失函数\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.09003, 0.24473, 0.66524])\n",
      "tensor([0.09003, 0.24473, 0.66524])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([2, 3, 4])\n",
    "# 使用PyTorch内置的softmax函数\n",
    "softmax = a.softmax(dim=0)\n",
    "print(softmax)\n",
    "# 根据计算公式\n",
    "manual_softmax = torch.pow(math.e, a) / torch.pow(math.e, a).sum()\n",
    "print(manual_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.40761, -1.40761, -0.40761])\n",
      "tensor([-2.40761, -1.40761, -0.40761])\n"
     ]
    }
   ],
   "source": [
    "# 使用PyTorch内置的log_softmax函数\n",
    "log_softmax = a.log_softmax(dim=0)\n",
    "print(log_softmax)\n",
    "# 根据计算公式\n",
    "manual_log_softmax = (torch.pow(math.e, a) / torch.pow(math.e, a).sum()).log()\n",
    "print(manual_log_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.40761],\n",
      "        [-1.40761],\n",
      "        [-0.40761]])\n",
      "tensor([2.40761, 1.40761, 0.40761])\n",
      "nll_loss: tensor([-4.52000, -3.42000,  0.40761])\n"
     ]
    }
   ],
   "source": [
    "print(manual_log_softmax.unsqueeze(1))\n",
    "# 使用PyTorch内置的nll_loss计算损失，reduction默认为mean，这里为了演示效果设置为none\n",
    "# LongTensor([0, 0, 0])表示正确类别所在索引，这里只有一个元素，为了方便演示，所以设置成0\n",
    "nll_loss = F.nll_loss(manual_log_softmax.unsqueeze(1), torch.LongTensor([0, 0, 0]), reduction='none')\n",
    "print(nll_loss)\n",
    "# 手动计算nll_loss\n",
    "# 如果是多类别的\n",
    "tmp = torch.Tensor([[-2.40761, 4.52], \n",
    "                    [-1.40761, 3.42], \n",
    "                    [-0.40761, 0.123]])\n",
    "tmp_index = torch.LongTensor([1, 1, 0])\n",
    "print(\"nll_loss: {}\".format(F.nll_loss(tmp, tmp_index, reduction='none')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.51207)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵损失函数\n",
    "b = torch.Tensor([[1, 2], [0.5, 3], [0.9, 4]])\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss(b, torch.LongTensor([0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.26894, 0.73106],\n",
      "        [0.07586, 0.92414],\n",
      "        [0.04311, 0.95689]])\n",
      "tensor([[-1.31326, -0.31326],\n",
      "        [-2.57889, -0.07889],\n",
      "        [-3.14406, -0.04406]])\n",
      "tensor([[-1.31326, -0.31326],\n",
      "        [-2.57889, -0.07889],\n",
      "        [-3.14406, -0.04406]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.51207)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [0.5, 3], [0.9, 4]])\n",
    "label = torch.LongTensor([0, 1, 0])\n",
    "softmax = x.softmax(dim=1)\n",
    "print(softmax)\n",
    "\n",
    "log = softmax.log()\n",
    "print(log)\n",
    "\n",
    "# softmax和log其实可以合并为一步：\n",
    "log_softmax = x.log_softmax(dim=1)\n",
    "print(log_softmax)\n",
    "\n",
    "# CrossEntropy等价于nll_loss(log_softmax)等价于nll_loss(log(softmax))\n",
    "F.nll_loss(log, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--cuda'], dest='cuda', nargs=0, const=True, default=False, type=None, choices=None, help='enables CUDA training', metavar=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N', \n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', \n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=2, metavar='N', \n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR', \n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M', \n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', \n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', \n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--num-processes', type=int, default=5, metavar='N', \n",
    "                    help='how many training processes to use (default: 5)')\n",
    "parser.add_argument('--cuda', action='store_true', default=False, \n",
    "                    help='enables CUDA training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N] [--epochs N] [--lr LR] [--momentum M] [--seed S] [--log-interval N] [--num-processes N] [--cuda]\n",
      "\n",
      "PyTorch MNIST\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  --batch-size N       input batch size for training (default: 64)\n",
      "  --test-batch-size N  input batch size for testing (default: 1000)\n",
      "  --epochs N           number of epochs to train (default: 10)\n",
      "  --lr LR              learning rate (default: 0.01)\n",
      "  --momentum M         SGD momentum (default: 0.9)\n",
      "  --seed S             random seed (default: 1)\n",
      "  --log-interval N     how many batches to wait before logging training status\n",
      "  --num-processes N    how many training processes to use (default: 5)\n",
      "  --cuda               enables CUDA training\n"
     ]
    }
   ],
   "source": [
    "parser.print_help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, args, model, device, dataloader_kwargs):\n",
    "    # 手动设置随机种子\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    # 加载训练数据\n",
    "    train_loader = DataLoader(datasets.MNIST('./pytorch_dataset/', train=True, download=True, \n",
    "                                             transform=Compose([ToTensor(), Normalize((0.,), (1,))])), \n",
    "                              batch_size=args.batch_size, shuffle=True, num_workers=1, **dataloader_kwargs)\n",
    "    # 使用随机梯度下降进行优化\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    # 开始训练，训练epochs\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_epoch(epoch, args, model, device, train_loader, optimizer)\n",
    "        \n",
    "        \n",
    "def train_epoch(epoch, args, model, device, data_loader, optimizer):\n",
    "    # 模型转换为训练模式\n",
    "    model.train()\n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        # 优化器梯度设置为0\n",
    "        optimizer.zero_grad()\n",
    "        # 输入特征预测值\n",
    "        output = model(data.to(device=device))\n",
    "        # 用预测值与标准值计算损失\n",
    "        loss = F.nll_loss(output, target.to(device=device))\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新梯度\n",
    "        optimizer.step()\n",
    "        # 每100个小批次打印一下日志\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format\n",
    "                  (pid, epoch, batch_idx * len(data), len(data_loader.dataset), \n",
    "                   100. * batch_idx / len(data_loader), loss.item()))\n",
    "            \n",
    "\n",
    "def test(args, model, device, dataloader_kwargs):\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(args.seed)\n",
    "    # 加载测试数据\n",
    "    test_loader = DataLoader(datasets.MNIST('./pytorch_dataset/', train=False, \n",
    "                                            transform=Compose([ToTensor(), Normalize((0.,), (1.,))])), \n",
    "                             batch_size=args.test_batch_size, shuffle=True, num_workers=1, \n",
    "                             **dataloader_kwargs)\n",
    "    # 运行测试\n",
    "    test_epoch(model, device, test_loader)\n",
    "    \n",
    "    \n",
    "def test_epoch(model, device, data_loader):\n",
    "    # 将模型转换为测试模式\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data.to(device))\n",
    "            test_loss += F.nll_loss(output, target.to(device), reduction='sum').item()\n",
    "            # 得到概率最大的索引\n",
    "            pred = output.max(1)[1]\n",
    "            # 如果预测的索引和目标索引相同，则认为预测正确\n",
    "            correct += pred.eq(target.to(device)).sum().item()\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
    "          (test_loss, correct, len(data_loader.dataset), 100. * correct / len(data_loader.dataset)))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 解析参数\n",
    "    args = parser.parse_args()\n",
    "    # 判断是否使用GPU设备\n",
    "    use_cuda = args.cuda and torch.cuda.is_available()\n",
    "    # 运行时设备\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu:0\")\n",
    "    # 使用固定缓冲区\n",
    "    dataloader_kwargs = {\"pin_memory\": True} if use_cuda else {}\n",
    "    # 多进程训练，Windows环境使用'spawn'\n",
    "    mp.set_start_method('spawn')\n",
    "    # 将模型复制到GPU\n",
    "    model = MNISTNet().to(device=device)\n",
    "    # 多进程共享模型参数\n",
    "    model.share_memory()\n",
    "    processes = []\n",
    "    for rank in range(args.num_processes):\n",
    "        p = mp.Process(target=train, args=(rank, args, model, device, dataloader_kwargs))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()  # 使主线程（是主线程还是主进程？？？）阻塞，等待子线程执行完成再执行\n",
    "    # 测试模型\n",
    "    test(args, model, device, dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10865\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.313203\n",
      "10869\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.313928\n",
      "10867\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.312402\n",
      "10866\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.319616\n",
      "10868\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.298651\n",
      "10865\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.333352\n",
      "10868\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.199292\n",
      "10867\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.216352\n",
      "10866\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.172152\n",
      "10869\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.389586\n",
      "10868\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.196595\n",
      "10867\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.292793\n",
      "10865\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.292164\n",
      "10866\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.122017\n",
      "10869\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.227576\n",
      "10868\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.207654\n",
      "10867\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.143153\n",
      "10866\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.187303\n",
      "10869\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.344576\n",
      "10865\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.219935\n",
      "10868\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.105733\n",
      "10867\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.220491\n",
      "10866\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.160373\n",
      "10869\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.257645\n",
      "10865\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.273021\n",
      "10866\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.111925\n",
      "10867\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.087650\n",
      "10868\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.325848\n",
      "10869\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.106938\n",
      "10865\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.249374\n",
      "10866\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.264284\n",
      "10868\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.119351\n",
      "10867\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.108423\n",
      "10869\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.127540\n",
      "10865\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.164688\n",
      "10866\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.413981\n",
      "10868\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.166465\n",
      "10867\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.249529\n",
      "10869\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.243912\n",
      "10865\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.225783\n",
      "10868\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.103957\n",
      "10866\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.269112\n",
      "10867\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.109019\n",
      "10865\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.160134\n",
      "10869\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.074050\n",
      "10868\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091954\n",
      "10866\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.075680\n",
      "10867\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.122908\n",
      "10865\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.100553\n",
      "10869\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.137837\n",
      "10866\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.217393\n",
      "10868\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.061089\n",
      "10865\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.101145\n",
      "10867\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.081895\n",
      "10869\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.167007\n",
      "10866\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.120513\n",
      "10868\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.115590\n",
      "10865\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.348818\n",
      "10869\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.081525\n",
      "10867\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.079862\n",
      "10866\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.034191\n",
      "10868\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.158381\n",
      "10865\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.069928\n",
      "10869\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.184978\n",
      "10867\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.083305\n",
      "10866\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.078443\n",
      "10868\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.079240\n",
      "10865\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.111862\n",
      "10867\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.190236\n",
      "10869\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.032801\n",
      "10868\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.087613\n",
      "10866\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.099782\n",
      "10865\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.085743\n",
      "10869\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.113930\n",
      "10867\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.108538\n",
      "10868\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.136988\n",
      "10866\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.097569\n",
      "10865\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.128101\n",
      "10867\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.155289\n",
      "10869\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.131106\n",
      "10868\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.127445\n",
      "10866\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.145171\n",
      "10865\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.294321\n",
      "10867\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.081418\n",
      "10869\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.097399\n",
      "10868\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.039687\n",
      "10866\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.129090\n",
      "10865\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.077534\n",
      "10867\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.096268\n",
      "10869\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.118091\n",
      "10868\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.281612\n",
      "10866\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.145629\n",
      "10865\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.160233\n",
      "10869\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.142738\n",
      "10867\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.024665\n",
      "10868\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.124792\n",
      "10866\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.106599\n",
      "10865\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.138020\n",
      "10869\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.123801\n",
      "10867\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.169930\n",
      "\n",
      "Test set: Average loss: 0.1097, Accuracy: 9676/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 mnist_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算f(x)=e^x的10阶麦克劳林展开的近似值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.pow: tensor([ 2.71828,  7.38906, 20.08554])\n",
      "mcLaughlin_ex: tensor([ 2.71828,  7.38899, 20.07967])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch计算f(x)=e^x的10阶麦克劳林展开的近似值\n",
    "x = torch.Tensor([1, 2, 3])\n",
    "print(\"torch.pow:\", torch.pow(math.e, x))\n",
    "\n",
    "\n",
    "# 10阶麦克劳林公式近似计算e^x\n",
    "def mcLaughlin_ex(x):\n",
    "    _sum = 1\n",
    "    for i in range(1, 11):\n",
    "        _sum += x ** i / math.factorial(i)\n",
    "    return _sum\n",
    "\n",
    "\n",
    "print(\"mcLaughlin_ex:\", mcLaughlin_ex(x))\n",
    "# 由此可知，10阶麦克劳林展开式已经能够很好地近似原函数\n",
    "# 阶乘惩罚---既能保留低次项对附近点的表达能力，又能避免高次项对低次项的压制，并且不影响高次项对更远趋势的预测能力"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
